#+TITLE: Cloud-Native Movie Review Platform
#+AUTHOR: Twilight4

* Table of Contents :toc:
- [[#project-overview][Project Overview]]
  - [[#full-project-directory-structure][Full Project Directory Structure]]
  - [[#full-architecture-diagram][Full Architecture Diagram]]
  - [[#project-scope--technologies-used][Project Scope / Technologies Used]]
  - [[#prerequisites][Prerequisites]]
  - [[#infrastructure--deployment-model][Infrastructure & Deployment Model]]
- [[#deploy-cloud-infrastructure][Deploy Cloud Infrastructure]]
  - [[#terraform-resources-overview][Terraform resources overview]]
  - [[#1-authenticate-and-set-up-gcp][1. Authenticate and set up GCP]]
  - [[#2-create-terraform-state-backend][2. Create Terraform State Backend]]
  - [[#3-manual-infrastructure-deployment-cd-managed-in-production][3. Manual infrastructure deployment (CD-managed in production)]]
  - [[#4-manual-application-of-manifests-cd-managed-in-production][4. Manual application of manifests (CD-managed in production)]]
  - [[#5-verify-deployment][5. Verify deployment]]
  - [[#6-clean-up-infrastructure][6. Clean up infrastructure]]
- [[#roadmap][Roadmap]]

* Project Overview
This project outlines every stage of the DevOps lifecycle, the tools and how the pieces connect. Everything fully automated end-to-end.

This project demonstrates a full production-grade DevOps setup using modern tooling:
- [[#Containerization][Containerization: Docker]]
- [[#Container-Orchestration][Container Orchestration: Kubernetes + Helm]]
- [[#Cloud-Infrastructure][Cloud Technologies (GCP): GKE, GCR, NoSQL Firestore DB]]
- [[#Cloud-Infrastructure][Infrastructure provisioning: Terraform]]
- [[#CICD][CI/CD pipeline: Github Actions]]
- [[#GitOps][GitOps: ArgoCD]]
- [[#Monitoring-and-Logging][Monitoring and Logging: Prometheus, Grafana]]

** Full Project Directory Structure
#+begin_src shell
full-devops-cycle-project/
├── app/                → Application source code + Dockerfile
│   ├── api-node/
│   │   ├── tests/
│   │   │   └── movies.test.js
│   │   ├── src/
│   │   │   ├── db/
│   │   │   ├── routes/
│   │   │   ├── app.js
│   │   │   └── server.js
│   │   ├── Taskfile.yaml
│   │   ├── package.json
│   │   ├── package-lock.json
│   │   └── Dockerfile
│   ├── Taskfile.yaml
│   └── README.md
│
├── k8s/                → Deployment manifests for multi-env setup
│   ├── overlays/
│   │   ├── staging/
│   │   │   ├── api-node/
│   │   │   │   ├── patches/
│   │   │   │   └── kustomization.yaml
│   │   │   └── kustomization.yaml
│   │   └── production/
│   │       ├── api-node/
│   │       │   ├── patches/
│   │       │   └── kustomization.yaml
│   │       └── kustomization.yaml
│   ├── helm/
│   │   ├── api-node-helm-chart/
│   │   │   ├── templates/
│   │   │   ├── values.yaml
│   │   │   ├── values-staging.yaml
│   │   │   └── Chart.yaml
│   │   ├── argocd-application/       → ArgoCD installation with helm for CD pipeline
│   │   │   ├── templates/
│   │   │   ├── values.yaml
│   │   │   ├── values-staging.yaml
│   │   │   └── Chart.yaml
│   │   └── Taskfile.yaml
│   ├── deploy-services/
│   │   ├── api-node/
│   │   │   ├── namespaces/
│   │   │   ├── manifests/
│   │   │   ├── hostname-staging.yaml
│   │   │   └── hostname-production.yaml
│   │   └── Taskfile.yaml
│   └── Taskfile.yaml
│
├── k8s-cluster-local/      → Docs for Deploying cluster locally
│   ├── kind-bind-mount-2
│   ├── kind-bind-mount-1
│   ├── Taskfile.yaml
│   └── kind-config.yaml.TEMPLATE
│
├── terraform/          → Infrastructure as Code (GCP)
│  ├── modules/
│  │   ├── gke/
│  │   │   ├── variables.tf
│  │   │   ├── outputs.tf
│  │   │   └── main.tf
│  │   └── firestore/
│  │       ├── variables.tf
│  │       └── main.tf
│  ├── envs/
│  │   ├── staging/
│  │   │   ├── variables.tf
│  │   │   ├── terraform.tfvars
│  │   │   ├── main.tf
│  │   │   ├── providers.tf
│  │   │   ├── versions.tf
│  │   │   ├── resources.tf
│  │   │   └── backend.tf
│  │   ├── production/
│  │   │   ├── variables.tf
│  │   │   ├── terraform.tfvars
│  │   │   ├── main.tf
│  │   │   ├── providers.tf
│  │   │   ├── versions.tf
│  │   │   ├── resources.tf
│  │   │   └── backend.tf
│  │   └── Taskfile.yaml
│  ├── tests/
│  │   └── bash/
│  │       ├── test-staging.sh
│  │       └── test-production.sh
│  ├── Taskfile.yaml
│  ├── delete-backend.sh
│  └── backend.sh
│
├── cicd/            → GitOps - ArgoCD configuration
│   ├── github-actions/
│   │   └── Taskfile.yaml
│   ├── argocd-gitops/
│   │   ├── namespaces/
│   │   │   └── argocd.yaml
│   │   ├── clusters/
│   │   │   ├── staging/
│   │   │   │   └── application.yaml
│   │   │   └── production/
│   │   │       └── application.yaml
│   │   └── Taskfile.yaml
│   └── Taskfile.yaml
│
├── monitoring/         → Node + monitoring configuration
│   ├── grafana-dashboards/
│   └── prometheus/
│
├── .github/            → CI/CD pipelines
│   └── workflows/
│       ├── ci.yaml
│       └── cd.yaml
│
├── Taskfile.yaml
└── README.org
#+end_src

** Full Architecture Diagram
#+begin_src shell
                                    ┌──────────────────────────┐
                                    │        Developers        │
                                    └───────────┬──────────────┘
                                                │ git push
                                                ▼
                                      ┌────────────────────┐
                                      │     GitHub Repo    │
                                      └─────────┬──────────┘
                                                │ triggers
                                                ▼
                                  ┌────────────────────────────────┐
                                  │         CI Pipeline            │
                                  └─────────────┬──────────────────┘
                                                │
                                                ▼
                                       ┌────────────────────┐
                                       │ Build Docker Image │
                                       └────────────────────┘
                                                │ push
                                                ▼ 
                                      ┌──────────────────────┐
                                      │ GCR (Artifact Reg.)  │
                                      └─────────┬────────────┘
                                                │ pull request triggers CD pipeline
                                                ▼
                                ┌─────────────────────────────────┐
                                │  CD Pipeline (Terraform / Helm) │
                                └───────────────┬─────────────────┘
                                                │ 
                                                ▼
                            ┌──────────────────────────────────────────┐
                            │  Terraform: Deploys Cloud Infrastructure │
                            └───────────────────┬──────────────────────┘
                                                │
                                                ▼
                                ┌─────────────────────────────────┐
                                │  Bash Test Script Validation    │
                                └───────────────┬─────────────────┘
                                                │ Testing
                                                ▼
                                ┌──────────────────────────────────────┐
                                │ Helm/ArgoCD: Deploy Manifests in GKE │
                                └───────────────┬──────────────────────┘
                                                │
                                                ▼
                      ┌──────────────────────────────────────────────────────┐
                      │                GKE Cluster w/ ArgoCD                 │
                      │                                                      │
                      │   ┌────────────┐    ┌─────────────────────────────┐  │
                      │   │ Deployment │--▶│  Horizontal Pod Autoscaler  │  │
                      │   └────────────┘    └─────────────────────────────┘  │
                      │          │                 │                         │
                      │          ▼                 ▼                         │
                      │   ┌────────────┐      ┌──────────────┐               │
                      │   │   Service  │◀──▶│   Ingress    │               │
                      │   └────────────┘      └──────────────┘               │
                      │          │                                           │
                      │          ▼                                           │
                      │   ┌────────────┐                                     │
                      │   │    Pods    │                                     │
                      │   └────────────┘                                     │
                      │                            ArgoCD watches Git repo   │
                      └─────────────────────────┬────────────────────────────┘
                                                │
                                                │
                                                ▼
                                   ┌───────────────────────────┐
                                   │   GCP NoSQL Firestore DB  │
                                   └────────────┬──────────────┘
                                                │
                                                │
                                                ▼
                                   ┌──────────────────────────┐
                                   │ Monitoring Stack on GKE  │
                                   │ - Prometheus             │
                                   │ - Grafana                │
                                   └──────────────────────────┘

#+end_src

** Project Scope / Technologies Used
*** Containerization
- Dockerfile
- Image optimization

*** Container Orchestration
- Deployments, Services, Ingress
- ConfigMaps and Secrets
- Horizontal Pod Autoscaler (HPA)
- Namespaces and environment separation
- Network policy
- Liveness/readiness probes
- Kustomize overlays
- Helm chart packaging and templating with namespace/env separation

*** Cloud Infrastructure
Terraform provisions GCP infrastructure:
- GKE cluster
- GCS bucket for tfstate backend
- GCP Firestore NoSQL Database
- VPC, subnets, and firewall rules
- Artifact Registry (GCR)
- IAM roles and service accounts
- Cloud Storage buckets

The Terraform Bash test script:
- Uses Terraform outputs to dynamically determine application endpoints
- Supports ApplicationSet for cluster-wide deployment orchestration
- Performs end-to-end validation with a health-check URL to ensure applications are fully functional

*** CI/CD
- Build, test pipelines using GitHub Actions
- Docker image building and pushing to GitHub Container Registry (GHCR)
- Environment-specific deployments for staging and production
- Automated application deployment pipeline, including:
  + Terraform infrastructure provisioning for GKE clusters
  + Helm chart rendering and deployment via ArgoCD =ApplicationSet=
  + Health checks to validate service availability
- Automated cleanup of temporary resources after test runs

*** GitOps
- Automated Kubernetes deployments managed with ArgoCD
- Declarative Helm charts for applications and environment-specific configuration
- Git-driven versioning and rollback of deployments

*** Monitoring and Logging
- Prometheus: metrics
- Grafana: dashboards

*** Security
- Kubernetes network policy rules:
  + Allow traffic only from the Ingress Controller
  + Deny ALL other internal pod traffic
  + Allow outbound internet (API needs GCP Firestore)
- Kubernetes Secret Management
- Terraform IAM least-privilege model

** Prerequisites
- git
- go-task
- docker
- kubectl
- argocd
- kubectx
- minikube or kind
- helm (v3)
- terraform
- GCP subscription
- google-cloud-cli
- google-cloud-cli-gsutil
- google-cloud-cli-component-gke-gcloud-auth-plugin

** Infrastructure & Deployment Model
*** Important note for users testing this configuration
- All GKE clusters and Kubernetes manifests are *fully managed by the CD pipeline*
- Manual deployment is *NOT required* for normal operation ([[#3-manual-cluster-deployment-cd-managed-in-production][Step 3]] and [[#4-manual-application-of-manifests-cd-managed-in-production][Step 4]])
- The instructions below exist for:
  - local development / learning
  - debugging
  - reproducing CI/CD behavior manually

The CD pipeline is the *source of truth*. Manual steps should be treated as *optional*.

*** CI/CD Pipeline Prerequisites
If you want to use the *pre-configured CI/CD pipeline* in this repository, you must define the following *GitHub repository variables and secrets* before running it:

*Secrets*:
- =GCP_SA_KEY= – Service Account key used by Terraform/google-cloud-cli
- =GKE_CLUSTER_NAME= - Cluster name used by google-cloud-cli
- =GKE_REGION= - Region used by google-cloud-cli
- =GCP_PROJECT_ID= - Project name used by google-cloud-cli
- =GHCR_TOKEN= – Token with permission to push/pull images from GHCR
- =PR_ACTION= – Token used by the PR automation workflow

*Repository Variables*:
- =GH_USERNAME= – GitHub username associated with the GHCR account

Without these values configured, the CI/CD pipeline will fail.

* Deploy Cloud Infrastructure
** Terraform resources overview
Terraform will create resources in the following sequence:
1. VPC network
2. VPC subnet
3. firewall rules
4. GKE cluster

** 1. Authenticate and set up GCP
#+begin_src shell
# Change values in root Taskfile.yaml for CLUSTER_NAME, GCP_REGION, GCP_ZONE

# Authenticate to Google Cloud
gcloud auth application-default login

# Create a project
gcloud projects create <PROJECT_NAME>
# Change to another project
gcloud config set project <PROJECT_ID>
# List projects
gcloud projects list

# Set up billing account for your project
## List billing accounts
gcloud billing accounts list
## Check if billing is enabled for a projec
gcloud billing projects describe <PROJECT_ID>
## Link a billing account to a project
gcloud billing projects link <PROJECT_ID> --billing-account=<ACCOUNT_ID>

# Manually enable critical APIs which can not be enabled by Terraform unless they are already enabled
go-task gcp:02-enable-apis 

# Set the default region and zone
gcloud config set compute/region <GCP_REGION>
gcloud config set compute/zone <GCP_ZONE>

# Verify configuration
gcloud config configurations list
#+end_src

** 2. Create Terraform State Backend
Terraform needs a remote backend to store its state.

The =backend.sh= script creates the following resources for currently set project:
- GCS bucket with enabled bucket versioning
- dedicated service account with the privileges: storage.admin and viewer
- key for the Terraform backend

The SA and key are created for the purpose of:
- CI/CD automation (Stable, non-expiring credentials)
- Least privilege access
- Auditability
- Stable & central
- Avoid using user credentials in automation (Actions are logged under the SA, not a human user)

#+begin_src shell
# Modify the LOCATION variable and PROJECT_ID to yours and optionally other vars before executing
./backend.sh

# Store the exported terraform-sa-key.json in a secure place in your local machine e.g. ~/.gcp/terraform-sa-key.json
# The set the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to it (for persistence save in shell profile e.g. .zshrc)
export GOOGLE_APPLICATION_CREDENTIALS="$HOME/.gcp/terraform-sa-key.json"

# If you want to clean up the created resources by backend.sh
PROJECT_ID=<my-project> BUCKET_NAME=<tfstate-12345> ./delete-backend.sh

# To view the created resources
gcloud config get-value project                             # Outputs the current project ID set in gcloud
gcloud storage buckets list --project <PROJECT_ID>          # Check if the bucket exists
gcloud iam service-accounts list --project <PROJECT_ID>     # List all service accounts in the project

# Check the bound iam policies for the created service account
gcloud projects get-iam-policy <PROJECT_ID> \
    --flatten="bindings[].members" \
    --format="table(bindings.role, bindings.members)" \
    --filter="bindings.members:<SA_NAME>@<PROJECT_ID>.iam.gserviceaccount.com"

# NOTE: You don't need to authenticate gcloud using that key
# Terraform will automatically pick up the exported 'GOOGLE_APPLICATION_CREDENTIALS' value
# and execute actions (init, plan, apply) as the SA account
# Authenticate if you want to run gcloud commands as the SA for (debugging or verifying permissions)
gcloud auth activate-service-account --key-file=<KEY_FILE>
# Check which account is active (The * indicates the active account)
gcloud auth list
#+end_src

Then update backend.tf's =bucket= argument to point to the =<BUCKET_NAME>=.

** 3. Manual infrastructure deployment (CD-managed in production)
WARNING:
If the GKE cluster is deleted manually via the GCP Console or =gcloud=, Terraform state will drift. If you manually delete the cluster, run:
- =terraform state rm google_container_cluster.<name>=

*** Local Systems
**** Minikube
#+begin_src shell
# View task commands
cd k8s-cluster-local
go-task --list-all

# Start cluster 
go-task minikube:01-create-cluster

# Enable ingress add-on
go-task minikube:02-enable-ingress

# Verify
go-task minikube:03-check-status

# Delete the cluster
go-task minikube:04-delete-cluster
#+end_src

**** KinD
The =kind-config.yaml.TEMPLATE= is a template for generating =kind-config.yaml= with =go-task kind:01-generate-config= command which uses 2 extra mounts that the kind cluster is going to mount in to each node for persisting data.

#+begin_src shell
# View task commands
cd k8s-cluster-local
go-task --list-all

# Start cluster with 2 worker nodes
go-task kind:01-generate-config        # it is going to create one container for control-plane and two containers for worker-nodes
go-task kind:02-create-cluster

# Verify
kubectl get nodes

# The cloud-provider-kind tool enables to run load-balancers within the kind cluster so we'll be able to access it from the host
# when we'll deploy a service with kind: LoadBalancer, install it when you want to deploy LoadBalancer services
#go-task kind:03-run-cloud-provider-kind  

# Delete the cluster
go-task kind:04-delete-cluster
#+end_src

**** Firestore authentication
If you're deploying cluster locally, you must also enable the Firestore database manually and authenticate it with the movie-api service.

Create a JSON key and put in a secret manifest:
#+begin_src shell
# Create service account
gcloud iam service-accounts create firestore-sa --display-name="Firestore access for movie-api"

# Grant permissions
gcloud projects add-iam-policy-binding movie-review-platform8451 \
  --member="serviceAccount:firestore-sa@movie-review-platform8451.iam.gserviceaccount.com" \
  --role="roles/datastore.user"

# Create a JSON key
gcloud iam service-accounts keys create sa.json \
  --iam-account=firestore-sa@movie-review-platform8451.iam.gserviceaccount.com

# Create the secret for the SA account
kubectl create secret generic firestore-sa \
  --from-file=adc.json=./sa.json \
  -n staging

# Verify
kubectl get secret firestore-sa -n staging
#+end_src

Update your deployment manifest:
#+begin_src yaml
spec:
  template:
    spec:
      volumes:
        - name: gcp-creds
          secret:
            secretName: firestore-sa

containers:
  - name: movie-api
    image: ghcr.io/twilight4/api-node
    env:
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /gcp/adc.json
      - name: FIRESTORE_PROJECT_ID
        value: movie-review-platform8451
      - name: FIRESTORE_COLLECTION
        value: movies
    volumeMounts:
      - name: gcp-creds
        mountPath: /gcp
        readOnly: true
#+end_src

Then you can re-apply the deployment manifest:
#+begin_src shell
kubectl apply -k k8s/overlays/staging
#+end_src

Also you must create the [[#Deploy-the-Firestore-database-using-=google-cloud-cli=][GCP Firestore]] DB itself.

**** Test health endpoint
#+begin_src shell
# Port-forward the service
kubectl port-forward svc/<SVC_NAME> 3000:80 

# OR check from inside of the container
kubectl exec -it <POD_NAME> -- netstat -tlnp           # to sanity-check which port the service is running on
kubectl exec -it <POD_NAME> -- wget -qO- http://localhost:3000/health

xh localhost:3000/health
# Expected output: {"status":"ok"}
#+end_src

**** [[#Verify-the-firestore-database-is-working-as-expected][Verify the firestore database is working as expected]]

*** Test and deploy infrastructure using Terraform
**** Testing Terraform infrastructure before deployment
This project includes a Bash-based test script to validate the Terraform configuration by deploying infrastructure to GKE, applying ArgoCD-managed GitOps applications, and verifying that the service behaves as expected.

The CD pipeline also includes testing for the production environment if a pull request is issued.

***** Prerequisites
Before running the tests, ensure you have:
- Terraform installed
- Valid GCP credentials available in your environment
- Permissions (Terraform backend configured) to create and destroy GKE-related resources
- go-task installed and configured to run ArgoCD tasks
- Helm charts and manifests for the application available in the repository

***** Running the Tests
#+begin_src shell
# Navigate to the Bash test directory
cd terraform/tests/bash

# Run the script (staging environment)
./test-staging.sh
#+end_src

***** What the Test Script Does
The script performs the following steps:
1. *Configures Terraform execution*
   - Changes to the specified Terraform environment
   - Optionally checks and applies formatting via =terraform fmt=
   - Validates configuration using =terraform validate=
   - Generates a Terraform plan with =terraform plan=

2. *Initializes and applies Terraform*
   - Runs =terraform init= and =terraform apply=
   - Provisions all required infrastructure on GKE

3. *Registers automatic cleanup*
   - Ensures terraform destroy is executed at the end of the script via trap
   - Cleanup runs even if the script fails

4. *Applies Helm GitOps Application via ArgoCD*
   - Renders environment-specific manifests using =go-task argocd:01-render-<env>=
   - Installs or upgrades the ArgoCD ApplicationSet (cluster-wide) via =go-task argocd:00-install-argocd-applicationset=
   - Deploys environment-specific manifests using =go-task argocd:02-install-<env>=
   - Ensures the application is deployed and managed via GitOps

5. *Fetches Terraform outputs*
   - Reads the =healthcheck_url= output from Terraform
   - This URL represents the exposed application endpoint (supports HTTP or HTTPS)

5. *Validates the deployed application*
   - Sends HTTP(S) GET requests to the endpoint
   - Retries up to 60 times with a 10-second delay between attempts
   - Confirms:
     - HTTP status code is =200=
     - Application responds with =ok=

6. *Prints response output*
   - Logs HTTP response and status code for debugging and visibility

***** Test Success Criteria
The test passes if:
- Terraform applies successfully
- The ArgoCD-managed application is deployed to GKE
- The application endpoint becomes reachable
- The HTTP response returns status =200= and body =ok=

If any of these conditions fail, the test fails and Terraform resources are destroyed automatically.

**** Deploying terraform infrastructure
#+begin_src shell
# Select environment to use
cd terraform/envs
# Change the values for terraform.tfvars to match your project_id, region, and zone

# View task commands from root directory
go-task --list-all

# Initialize terraform for production env
go-task terraform:01-init-production

# Validate and review the planned configuration
go-task terraform:02-validate-production
go-task terraform:03-plan-production

# Apply the terraform configuration
go-task terraform:04-apply-production

# Connect to the cluster
go-task gcp:08-connect-to-cluster
# Select cluster to use with kubectl commands
kubectx
#+end_src

*** Deploy GKE using =google-cloud-cli=
#+begin_src shell
# View task commands from root directory
go-task --list-all

# Create the GCP network, subnet, firewall rules, and cluster in sequence and connect to the GKE cluster
go-task gcp:01-init-cli          # to run gcloud init for automated setup
go-task gcp:07-create-all
go-task gcp:08-connect-to-cluster

# NOTE: If you have deployed additional resources (such as load balancers) that reference the VPC/Subnets/Etc... 
# you may need to manually clean up those resources in order for 'go-task gcp:09-clean-up' command to succeed. 
# You should also verify that you have cleaned up all resources to avoid unwanted costs.

# Connect to the cluster
go-task gcp:08-connect-to-cluster
# Select cluster to use with kubectl commands
kubectx

# Verify created resoruces
go-task gcp:09-verify-all
#+end_src

*** Deploy the Firestore database using =google-cloud-cli=
#+begin_src shell
# Enable the Firestore API
go-task gcp:db:01-enable-api

# Create the Firestore database
# Firestore is global per project, but you still must create it once
go-task gcp:db:02-create-db

# Verify Firestore exists
go-task gcp:db:03-list-db
#+end_src

** 4. Manual application of manifests (CD-managed in production)
NOTE:
In production, manifests are applied automatically via GitOps (ArgoCD).
The following methods are provided for local testing and manual validation only.

*** Using kubectl/kustomize
#+begin_src shell
# Navigate to Taskfile.yaml directory
cd k8s/deploy-services
go-task --list-all

# Render production env
go-task 01-render-production

# Apply production env
go-task 02-deploy-production

# To delete production env
go-task 03-delete-production
#+end_src

*** Using helm
#+begin_src shell
# Navigate to Taskfile.yaml directory
cd k8s/helm
go-task --list-all

# Render production env
go-task 01-render-production

# Install production env
go-task 02-install-production

# List realeases
go-task 03-list

# Uninstall production env
go-task 04-uninstall-production
#+end_src

*** Using GitOps Setup (ArgoCD)

#+begin_src shell
# Navigate to gitops directory
cd cicd/

# View task commands from root directory
go-task --list-all

# Access argocd UI via port-forwarding the argocd-server
go-task 01-port-forward-webui

# The login is 'admin', to retrieve the password then login to the web UI
go-task 02-get-webui-password

# Deploy production cluster
go-task 03-deploy-production-cluster 

# Delete production cluster
go-task 04-delete-production-cluster
#+end_src

** 5. Verify deployment
*** Using [[https://console.cloud.google.com/][GCP Portal]]
**** Kubernetes Engine
Clusters/Node Pools:
- Navigate to Kubernetes Engine → Clusters
- Click on a specific cluster → Node pools tab

**** Compute Engine
VM Instances/Disks/Networks & Subnets/Firewall Rules:
- Navigate to Compute Engine → VM instances → Overview
- Navigate to Compute Engine → Disks
- Navigate to VPC network → VPC networks
- Navigate to VPC network → Subnetworks
- Navigate to VPC network → Firewall

**** Cloud Storage
Buckets:
- Navigate to Storage → Browser
- Shows all buckets in the project
- Click a bucket → Objects tab shows stored objects

**** VPC Networks
IP Addresses/Routes/Firewall Rules:
- Navigate to VPC network → External IP addresses
- Navigate to VPC network → Routes
- Navigate to VPC network → Firewall

**** IAM & Service Accounts
Service Accounts/IAM Policies/Roles:
- Navigate to IAM & Admin → Service Accounts
- Navigate to IAM & Admin → IAM

**** Firestore database
Database Resources:
- Navigate to Firestore → Data
- Navigate to Firestore → Indexes
- Navigate to Firestore → Rules

**** APIs & Services
Enabled APIs:
- Navigate to APIs & Services → Dashboard

*** Using =google-cloud-cli=
To quickly check most relevant created resources, run task:
#+begin_src shell
go-task gcp:09-verify-all
#+end_src

**** Kubernetes Engine: Clusters / Node Pools
#+begin_src shell
# Check if the cluster is running
gcloud container clusters list --region <REGION>

# Get info about the cluster
gcloud container clusters describe <CLUSTER_NAME> --region <REGION>

# List node pools in a cluster
gcloud container node-pools list --cluster <CLUSTER_NAME> --region <REGION>

# Get info about a specific node pool
gcloud container node-pools describe <NODE_POOL_NAME> --cluster <CLUSTER_NAME> --region <REGION>

# Kubernetes nodes
kubectl get nodes -o wide

# Kubernetes ingress
kubectl get ingress -A

# Cluster info
kubectl cluster-info

# Ingress
go-task gcp:10-wait-for-ingress
#+end_src

**** Compute Engine → Instances / Disks / Networks
#+begin_src shell
# List VM instances
gcloud compute instances list
# or filter by zone
gcloud compute instances list --zones <ZONE>

# List disks
gcloud compute disks list
#+end_src

**** Cloud Storage → Buckets
#+begin_src shell
# List all buckets in a project
gcloud storage buckets list || gsutil ls
# or for older versions
gcloud alpha storage buckets list

# Get details of a bucket
gcloud storage buckets describe gs://<BUCKET_NAME>
#+end_src

**** VPC Networks → IP addresses / Firewall
#+begin_src shell
# List networks
gcloud compute networks list

# List subnets
gcloud compute networks subnets list --region <REGION>

# List external IP addresses
gcloud compute addresses list

# List firewall rules (all networks)
gcloud compute firewall-rules list

# List routes
gcloud compute routes list

# List firestore databases
gcloud firestore databases list
#+end_src

**** Service Accounts
#+begin_src shell
# List service accounts
gcloud iam service-accounts list

# Get details about a specific service account
gcloud iam service-accounts describe <SA_EMAIL>

# List the IAM roles granted to a service account
gcloud projects get-iam-policy <PROJECT_ID> \
    --flatten="bindings[].members" \
    --format="table(bindings.role, bindings.members)" \
    --filter="bindings.members:<SA_NAME>@<PROJECT_ID>.iam.gserviceaccount.com"
#+end_src

*** Verify GKE api-node service
**** Verify the service is working as expected
#+begin_src shell
# GET /movies - Returns list of movies
xh http://<CLUSTER_IP>/movies

# POST /movies - Adds a movie
xh post http://<CLUSTER_IP>/movies title="Avatar 2" rating:=4
xh http://<CLUSTER_IP>/movies

# GET /health - For Kubernetes probes
xh get http://<CLUSTER_IP>/health
# Expected output: {"status":"ok"}
#+end_src

**** Verify the firestore database is working as expected
#+begin_src shell
# One-liner smoke test - If this returns a number → Firestore + K8s + API are all working
curl -s http://<CLUSTER_IP>:<PORT>/movies | jq 'length'

# Test your Node app against Firestore - save the 'id' key from the output
curl -X POST http://<CLUSTER_IP>:<PORT>/movies \
  -H "Content-Type: application/json" \
  -d '{"title":"Blade Runner","rating":5}'

# Get document by ID
curl http://<CLUSTER_IP>:<PORT>/movies/<ID>

# Query documents
curl "http://<CLUSTER_IP>:<PORT>/movies?minRating=4"

# Update a document
curl -X PUT http://<CLUSTER_IP>:<PORT>/movies/A1bC2dE3 \
  -H "Content-Type: application/json" \
  -d '{"title":"Interstellar","rating":4}'

# Delete a document
curl -X DELETE http://<CLUSTER_IP>:<PORT>/movies/A1bC2dE3
#+end_src

** 6. Clean up infrastructure
*** Terraform destroy / gcloud
#+begin_src shell
# Remove all terraform resources and avoid charges
cd terraform/
go-task terraform:05-destroy-staging
go-task terraform:07-clean-plan-staging

# If you created cluster and firestore database manually using gcloud, to delete those resoruces run
go-task gcp:09-clean-up
go-task gcp:db:04-delete-db
#+end_src

*** GCP-nuke
If infrastructure destruction fails and leaves behind unwanted GCP resources (e.g. due to partial Terraform destroys, state drift, or manual intervention), those resources may continue to incur cost.

As a last-resort cleanup option, you can use the [[https://github.com/ekristen/gcp-nuke][gcp-nuke]] to remove all resources from a GCP Project, use this tool when normal Terraform/gcloud cleanup has failed:
#+begin_src shell
# Authenticate gcp-nuke to GCP account (only supported via a Service Account either by Key or via Workload Identity)
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json

# Dry-run first (STRONGLY RECOMMENDED)
gcp-nuke run \
  --config test-config.yaml \
  --project-id <PROJECT_NAME> 

# Execute destructive cleanup
gcp-nuke run \
  --config test-config.yaml \
  --project-id <PROJECT_NAME> \
  --no-dry-run
#+end_src

* Roadmap
Docker application:
- [X] Write demo movie-review app in NodeJS using Express API
- [X] Implement app logic
- [X] Implement API Endpoints (/health, /movies for POST/GET requests)
- [X] Package app with Dockerfile
- [X] Implement image optimization best practices
- [X] Add GCP Firestore integration (env vars, CRUD, routes)

CI/CD:
- [X] Generating image tag
- [X] Building and pushing image to GHCR for multi-architecture
- [X] Update staging/production tags for push to main or release tag
- [X] Create Pull Request with image tag changes
- [X] Create the infrastructure using terraform
- [X] Implement checks for terraform CD deployment (fmt/validate/plan)
- [X] Apply the updated manifests into cluster

Kubernetes:
- [X] Create necessary manifest files (deployment, service, ingress)
- [X] Implement Horizontal Pod Autoscaler for movie-api deployment
- [X] Namespaces and environment separation (Dev → debug mode, Staging → test mode)
  + =namePrefix: staging-= for staging overlay
  + namespace overrides
  + patchesStrategicMerge
  + image overrides
- [X] Implement Kustomize overlays
- [X] Implement GCP Firestore integration (ConfigMap, serviceaccount)
- [X] Implement network policy for GCP database
- [X] Implement helm charts packaging and templating with namespace/env separation
- [X] Automated Kubernetes deployments with ArgoCD
- [X] Git-driven versioning and rollbacks 

Terraform:
- [X] backend config
- [X] IAM roles and service accounts
- [X] VPC
- [X] Subnet
- [X] GKE cluster
- [X] VM instances
- [X] Cloud Storage bucket
- [X] Security groups + rules
- [X] Application load balancer
- [X] GCP Firestore database

Grafana, Prometheus:
- [ ] 
